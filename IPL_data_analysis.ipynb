{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3542a99e-658a-4455-aedd-06ca588ede45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# IPL Data Analysis\n",
    "\"\"\"\n",
    "1. findspark: Utility to locate and initialize Spark installation\n",
    "2. SparkSession: Entry point for DataFrame and SQL functionality\n",
    "3. Window: For window aggregation operations\n",
    "4. sql.functions (F): Collection of built-in functions\n",
    "- SparkSession initialization with local mode\n",
    "- All cores utilization with local[*]\n",
    "- Application name setting for monitoring\n",
    "\"\"\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()  # This automatically finds Spark installation(local spark)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IPL Data Analysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Created Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08eeb84b-f864-43e0-9dfd-d8e2abd159bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "\"\"\"\n",
    "1. spark.read: DataFrame reader interface\n",
    "2. option(): Configures CSV reading options\n",
    "3. show(): Display DataFrame content\n",
    "\n",
    "- Loads deliveries.csv with header recognition\n",
    "- Initial preview of first 5 rows to verify data loading\n",
    "- No schema enforcement at this stage (auto-detection)\n",
    "\n",
    "- Headers are read from the first row\n",
    "- All columns are initially inferred as strings\n",
    "- Data preview helps in understanding the structure\n",
    "\"\"\"\n",
    "\n",
    "deliveries_df = spark.read.option('header', 'true').csv('data/deliveries.csv')\n",
    "print(\"\\n--- Initial Data Preview ---\")\n",
    "deliveries_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fa23ff0-fa53-4431-93ae-42e740b278cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check distinct match IDs\n",
    "\"\"\"\n",
    "1. select(): Column selection\n",
    "2. distinct(): Remove duplicates\n",
    "3. sort(): Order results\n",
    "4. F.col(): Column reference for operations\n",
    "\n",
    "- Identifies all unique matches in the dataset\n",
    "- Sorts match IDs in descending order\n",
    "- Helps understand the tournament coverage\n",
    "\n",
    "- Shows total number of matches\n",
    "- Helps identify specific matches for analysis\n",
    "- Useful for data validation and scope definition\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- Distinct Match IDs (Sorted Descending) ---\")\n",
    "deliveries_df.select('match_id').distinct().sort(F.col(\"match_id\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88d6da8b-8881-46ad-be77-3dfe9f646522",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check initial schema\n",
    "\"\"\"\n",
    "1. printSchema(): Displays DataFrame structure\n",
    "2. Data type inspection\n",
    "3. Column structure analysis\n",
    "\n",
    "- Shows the automatically inferred schema\n",
    "- Helps identify data type issues\n",
    "- Guides schema optimization needs\n",
    "- All columns initially loaded as strings\n",
    "- Need for proper data type conversion identified\n",
    "- Foundation for schema optimization\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- Initial Schema ---\")\n",
    "deliveries_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fa0f3a2-9bf1-4fef-83d8-85894f30caf2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define proper schema with correct data types\n",
    "\"\"\"\n",
    "1. StructField: Defines column properties\n",
    "2. IntegerType/StringType: Data type definitions\n",
    "3. List comprehension for schema creation\n",
    "4. StructType: Schema container\n",
    "- Identifies numeric columns\n",
    "- Creates optimized schema structure\n",
    "- Reloads data with proper types\n",
    "- Numeric columns converted to IntegerType\n",
    "- Other columns remain as StringType\n",
    "- Improves query performance and memory usage\n",
    "- Enables proper numeric operations\n",
    "\"\"\"\n",
    "\n",
    "int_col = ['match_id', 'inning', 'over', 'ball', 'batsman_runs',\n",
    "           'extra_runs', 'total_runs', 'is_wicket']\n",
    "\n",
    "fields = [StructField(col, IntegerType(), nullable=True) if col in int_col  \n",
    "          else StructField(col, StringType(), nullable=True) for col in deliveries_df.columns]\n",
    "\n",
    "# Reload with proper schema\n",
    "deliveries_df = spark.read.option('header', 'true').schema(StructType(fields)).csv('deliveries.csv')\n",
    "print(\"\\n--- Updated Schema ---\")\n",
    "deliveries_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f76ce48-b6fe-4776-8f5c-f8b95757507d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter for IPL Final 2024 (match_id = 1426312)\n",
    "\"\"\"\n",
    "1. filter(): SQL-like data filtering\n",
    "2. show(): Display results\n",
    "3. Boolean condition evaluation\n",
    "\n",
    "- Isolates specific match data (Final match)\n",
    "- Creates focused dataset for detailed analysis\n",
    "- First step in match-specific analysis\n",
    "- Uses match_id for precise filtering\n",
    "- Reduces data volume for analysis\n",
    "- Prepares for innings-level analysis\n",
    "\"\"\"\n",
    "\n",
    "ipl_final_df = deliveries_df.filter('match_id == 1426312')\n",
    "print(\"\\n--- IPL Final Match Data ---\")\n",
    "ipl_final_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81eb54cf-b9e7-45d9-8119-fcbf6d893272",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First Innings Analysis\n",
    "\"\"\"\n",
    "1. filter(): Innings-level filtering\n",
    "2. show(): Data preview\n",
    "3. Boolean condition for innings selection\n",
    "- Isolates first innings data\n",
    "- Creates foundation for batting/bowling analysis\n",
    "- Prepares for detailed scorecard creation\n",
    "- Focuses on first innings (inning == 1)\n",
    "- Sets up for detailed statistical analysis\n",
    "- Important for match progression analysis\n",
    "\"\"\"\n",
    "\n",
    "first_innings_batting = ipl_final_df.filter('inning == 1')\n",
    "print(\"\\n--- First Innings Data ---\")\n",
    "first_innings_batting.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fa85b13-0ee1-4376-9de5-2a7e92971359",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# BATTING SCORECARD\n",
    "\"\"\"\n",
    "1. groupBy(): Aggregation by batsman\n",
    "2. agg(): Multiple aggregation functions\n",
    "3. when(): Conditional counting\n",
    "4. withColumn(): Column creation\n",
    "5. Window functions: Determining batting order\n",
    "6. join(): Combining statistics with order\n",
    "- Calculates comprehensive batting statistics:\n",
    "  * Total runs scored\n",
    "  * Balls faced\n",
    "  * Boundaries (4s and 6s)\n",
    "  * Strike rate\n",
    "- Determines correct batting order\n",
    "- Creates complete batting scorecard\n",
    "\n",
    "1. Basic Statistics:\n",
    "   - Runs, balls, boundaries\n",
    "   - Strike rate calculation\n",
    "2. Batting Order:\n",
    "   - Uses over.ball for chronological order\n",
    "   - Window function for sequential numbering\n",
    "3. Final Scorecard:\n",
    "   - Joins stats with batting order\n",
    "   - Orders by batting position\n",
    "\"\"\"\n",
    "\n",
    "scorecard_df = first_innings_batting.filter(\"extras_type is NULL\").groupBy('batter').agg(\n",
    "    F.sum('batsman_runs').alias('runs'),\n",
    "    F.count('ball').alias('balls'),\n",
    "    F.count(F.when(first_innings_batting.batsman_runs == 4, 1)).alias('4s'),\n",
    "    F.count(F.when(first_innings_batting.batsman_runs == 6, 1)).alias('6s'),\n",
    "    F.round(F.sum('batsman_runs') * 100 / F.count('ball'), 2).alias('S/R')\n",
    ")\n",
    "\n",
    "# Get batting order\n",
    "batsman_order = first_innings_batting.withColumn(\n",
    "    'over-ball', \n",
    "    (F.concat(F.col(\"over\"), F.lit(\".\"), F.col(\"ball\"))).cast(FloatType())\n",
    ").groupBy(\"batter\").agg(\n",
    "    F.min(\"over-ball\").alias(\"order\")\n",
    ").orderBy(\"order\")\n",
    "\n",
    "batting_order_df = batsman_order.withColumn(\n",
    "    \"batting_order\", \n",
    "    F.row_number().over(Window.orderBy(\"order\"))\n",
    ")\n",
    "\n",
    "# Join batting stats with batting order\n",
    "batting_scorecard_final = scorecard_df.join(\n",
    "    batting_order_df, \n",
    "    on=['batter'], \n",
    "    how='inner'\n",
    ").select('batting_order', 'batter', 'runs', 'balls', '4s', '6s', 'S/R').orderBy('batting_order')\n",
    "\n",
    "print(\"\\n--- BATTING SCORECARD (First Innings) ---\")\n",
    "batting_scorecard_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "453b1cc2-6cf1-42d9-9051-fdfef9aa1f79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# BOWLING SCORECARD\n",
    "\"\"\"\n",
    "1. groupBy(): Aggregation by bowler\n",
    "2. Complex when conditions: Handling extras\n",
    "3. Mathematical operations: Overs calculation\n",
    "4. Column manipulation and renaming\n",
    "\n",
    "- Calculates detailed bowling statistics:\n",
    "  * Runs conceded (excluding extras)\n",
    "  * Overs bowled\n",
    "  * Wickets taken\n",
    "  * Economy rate\n",
    "- Handles special cases (extras)\n",
    "1. Runs Analysis:\n",
    "   - Total runs conceded\n",
    "   - Adjustment for extras\n",
    "2. Over Calculation:\n",
    "   - Converts balls to overs\n",
    "   - Proper format (overs.balls)\n",
    "3. Economy Calculation:\n",
    "   - Runs per over\n",
    "   - Rounded to 2 decimals\n",
    "\"\"\"\n",
    "\n",
    "scorecard_bowler_df = first_innings_batting.groupBy('bowler').agg(\n",
    "    F.sum('total_runs').alias('runs_conceded'),\n",
    "    F.sum((F.when((F.col(\"extras_type\") == \"legbyes\") | (F.col(\"extras_type\") == \"byes\"), \n",
    "                  F.col(\"extra_runs\")))).alias('not_by_bowler'),\n",
    "    F.count(F.when((F.col(\"extras_type\").isNull()) | (F.col(\"extras_type\") == \"legbyes\") | \n",
    "                   (F.col(\"extras_type\") == \"byes\"), 1)).alias('balls'), \n",
    "    F.count(F.when(F.col(\"is_wicket\") == 1, 1)).alias('W')\n",
    ")\n",
    "\n",
    "# Format bowling figures\n",
    "scorecard_bowler_df = scorecard_bowler_df.select(\n",
    "    F.col('bowler'),\n",
    "    F.concat(F.floor(F.col('balls') / 6), F.lit(\".\"), (F.col('balls') % 6)).alias('O'), \n",
    "    (F.col('runs_conceded') - F.coalesce(F.col('not_by_bowler'), F.lit(0))).alias('R'),\n",
    "    F.col('W'),\n",
    "    F.round((F.col('runs_conceded') / (F.col('balls') / 6)), 2).alias('Econ')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c68ab73-c2ab-402c-b79c-7242192fbf86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAIDEN OVERS CALCULATION\n",
    "\"\"\"\n",
    "1. Multiple groupBy operations\n",
    "2. Complex filtering conditions\n",
    "3. Chained DataFrame operations\n",
    "4. Left join for complete stats\n",
    "\n",
    "- Identifies maiden overs (0 runs in 6 balls)\n",
    "- Accounts for extras correctly\n",
    "- Integrates with bowling figures\n",
    "\n",
    "1. Over-level Analysis:\n",
    "   - Groups by bowler and over\n",
    "   - Counts balls and runs\n",
    "2. Maiden Over Criteria:\n",
    "   - 6 balls bowled\n",
    "   - No runs from bat\n",
    "   - Handles extras properly\n",
    "3. Final Integration:\n",
    "   - Joins with main bowling stats\n",
    "   - Completes bowling scorecard\n",
    "\"\"\"\n",
    "\n",
    "maiden_bowler_df = first_innings_batting.groupBy('bowler', 'over').agg(\n",
    "    F.sum('total_runs').alias('runs_conceded'),\n",
    "    F.count(F.col('over')).alias('balls'),\n",
    "    F.sum((F.when((F.col(\"extras_type\") == \"legbyes\") | (F.col(\"extras_type\") == \"byes\"), \n",
    "                  F.col(\"extra_runs\")))).alias('not_by_bowler')\n",
    ")\n",
    "\n",
    "maiden_bowler_df = maiden_bowler_df.withColumn(\n",
    "    'runs_by_bowler', \n",
    "    F.col('runs_conceded') - F.coalesce(F.col('not_by_bowler'), F.lit(0))\n",
    ")\n",
    "\n",
    "# Filter maiden overs (0 runs in 6 balls)\n",
    "maiden_bowler_df = maiden_bowler_df.filter(\n",
    "    (F.col('runs_by_bowler') == 0) & (F.col('balls') == 6)\n",
    ").groupBy('bowler').agg(\n",
    "    F.count('bowler').alias('M')\n",
    ")\n",
    "\n",
    "# Join bowling stats with maiden overs\n",
    "bowling_scorecard_final = scorecard_bowler_df.join(\n",
    "    maiden_bowler_df, \n",
    "    on=['bowler'], \n",
    "    how='left'\n",
    ").fillna(value=0).select('bowler', 'O', 'M', 'R', 'W', 'Econ')\n",
    "\n",
    "print(\"\\n--- BOWLING SCORECARD (First Innings) ---\")\n",
    "bowling_scorecard_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6049f49a-70a0-4bd0-ae81-e04c3de4abbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "\"\"\"\n",
    "1. show(): Display final results\n",
    "2. Multiple DataFrame displays\n",
    "3. Final scorecards presentation\n",
    "\n",
    "- Displays complete batting scorecard\n",
    "- Shows final bowling figures\n",
    "- Provides match summary\n",
    "- Complete first innings analysis\n",
    "- Both batting and bowling perspectives\n",
    "- Ready for further analysis or export\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- BATTING SCORECARD (First Innings) ---\")\n",
    "batting_scorecard_final.show()\n",
    "\n",
    "print(\"\\n--- BOWLING SCORECARD (First Innings) ---\")\n",
    "bowling_scorecard_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. spark.stop(): Properly close Spark session\n",
    "2. Resource cleanup\n",
    "3. Memory management\n",
    "- Ensures proper cleanup of resources\n",
    "- Prevents memory leaks\n",
    "\"\"\"\n",
    "\n",
    "# Stop Spark Session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "IPL data analysis",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python (DE venv)",
   "language": "python",
   "name": "de_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
