{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd825df1",
   "metadata": {},
   "source": [
    "# IPL Machine Learning Analysis\n",
    "\n",
    "This notebook focuses on predictive analysis using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52e79b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# For ML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IPL ML Analysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the data\n",
    "deliveries_df = spark.read.csv('data/deliveries.csv', header=True, inferSchema=True)\n",
    "matches_df = spark.read.csv('data/matches.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4629121",
   "metadata": {},
   "source": [
    "## 1. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfac282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features for score prediction\n",
    "def create_features_first_innings(df_deliveries, df_matches):\n",
    "    # Join dataframes\n",
    "    combined_df = df_deliveries.join(df_matches, df_deliveries.match_id == df_matches.id)\n",
    "    \n",
    "    # Calculate features for first 6 overs\n",
    "    powerplay_stats = combined_df.filter(F.col('over') <= 6) \\\n",
    "        .groupBy('match_id') \\\n",
    "        .agg(F.sum('total_runs').alias('powerplay_runs'),\n",
    "             F.sum('is_wicket').alias('powerplay_wickets'),\n",
    "             F.count(F.when(F.col('extras_type').isNotNull(), 1)).alias('powerplay_extras'))\n",
    "    \n",
    "    # Calculate final innings score\n",
    "    final_scores = combined_df.groupBy('match_id') \\\n",
    "        .agg(F.sum('total_runs').alias('final_score'))\n",
    "    \n",
    "    # Combine features\n",
    "    feature_df = powerplay_stats.join(final_scores, 'match_id')\n",
    "    \n",
    "    return feature_df\n",
    "\n",
    "# Create features for match winner prediction\n",
    "def create_features_match_winner(df_matches):\n",
    "    # Calculate team form (last 3 matches)\n",
    "    matches_pd = df_matches.toPandas()\n",
    "    matches_pd['team1_last3'] = matches_pd.groupby('team1')['winner'].transform(\n",
    "        lambda x: x.shift().rolling(3).mean())\n",
    "    matches_pd['team2_last3'] = matches_pd.groupby('team2')['winner'].transform(\n",
    "        lambda x: x.shift().rolling(3).mean())\n",
    "    \n",
    "    return matches_pd\n",
    "\n",
    "# Create feature datasets\n",
    "score_features = create_features_first_innings(deliveries_df, matches_df)\n",
    "# Fix: Ensure 'winner' is numeric before calculating rolling mean\n",
    "matches_pd = matches_df.toPandas()\n",
    "matches_pd['winner_numeric'] = (matches_pd['winner'] == matches_pd['team1']).astype(int)\n",
    "\n",
    "matches_pd['team1_last3'] = matches_pd.groupby('team1')['winner_numeric'].transform(\n",
    "    lambda x: x.shift().rolling(3).mean())\n",
    "matches_pd['team2_last3'] = matches_pd.groupby('team2')['winner_numeric'].transform(\n",
    "    lambda x: x.shift().rolling(3).mean())\n",
    "\n",
    "winner_features = matches_pd\n",
    "\n",
    "print(\"\\n--- Score Prediction Features Sample ---\")\n",
    "score_features.show(5)\n",
    "\n",
    "print(\"\\n--- Match Winner Features Sample ---\")\n",
    "print(winner_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ade535",
   "metadata": {},
   "source": [
    "## 2. Final Score Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8d95ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for score prediction\n",
    "score_data = score_features.toPandas()\n",
    "\n",
    "X = score_data[['powerplay_runs', 'powerplay_wickets', 'powerplay_extras']]\n",
    "y = score_data['final_score']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "score_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "score_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = score_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f'Root Mean Square Error: {rmse}')\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': score_model.feature_importances_\n",
    "})\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance.sort_values('importance', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a1bdec",
   "metadata": {},
   "source": [
    "## 3. Match Winner Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac9272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for winner prediction\n",
    "winner_data = winner_features.dropna()\n",
    "\n",
    "X = winner_data[['team1_last3', 'team2_last3', 'toss_winner']]\n",
    "y = winner_data['winner']\n",
    "\n",
    "# Convert categorical variables\n",
    "X = pd.get_dummies(X, columns=['toss_winner'])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "winner_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "winner_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = winner_model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model Accuracy: {accuracy}')\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f33941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark Session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DE venv)",
   "language": "python",
   "name": "de_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
